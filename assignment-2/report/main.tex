\documentclass[11pt]{article}
\input{preamble.tex}

\title{Web Data Management - Assignment 2 \\ Large-Scale Data Management with Hadoop}
\author{Friso Abcouwer - 4019873 \and Matthijs van Dorth - 1265911}

\begin{document}

\maketitle

\section{Introduction}
In this report, we will present our solutions to the exercises in Chapter 19 of the Web Data Management book.
Our Java code can be found in the /src folder in the zip-file we handed in, and our input and output files can be found in the /input and /output folders, respectively.

\section{Combiner Functions}
After implementing the example MapReduce job, making the Combiner was as simple as copying the Reducer and setting it in the Job class. (Looking back, we could also have simply set the Reducer as the Combiner.)
Without the Combiner, the Mapper will send pairs of the form $<author, 1>$ to the Reducer. However, the Combiner intercepts these and instead sends pairs of the form $<author,N>$ to the Reducer, reducing the amount of pairs that need to be sent to the Reducer because the intermediate result has already been generated.

\section{Movies}
For the Movies exercise, the first thing we had to do was handle the XML input in such a way that we could easily extract the information we needed from it.
For this, we used the JDOM package~\footnote{JDOM homepage: \url{http://jdom.org/}}, which provides an easy interface for extracting objects and values by name from XML files.
In our Mapper, this is first done for general movie information, then for information about actors, and finally for information about the director. Movie-actor output lines are all written to their own single key, as are movie-director output lines. In the Reducer, the values corresponding to the movie-actor key are written to the movie-actor output, and the values corresponding to the movie-director key are written to the movie-director output.

\section{PIGLatin Scripts}
The scripts we used can be found in Listings~\ref{lst:pig-ex1} through~\ref{lst:pig-ex5}.

\section{Inverted File}
For the inverted file project, we based our own solution on the tf-idf for Hadoop tutorial by Marcello de Sales~\footnote{\url{https://code.google.com/p/hadoop-clusternet/wiki/RunningMapReduceExampleTFIDF}}. To obtain our final result, we split the entire task up into 3 Hadoop jobs, which are run in sequence from the file \lstinline{IFJob.java}.

\begin{itemize}

\item \textbf{Job 1: Term Frequency -} In this job, the Mapper goes through all of the files in the input directory, and for each term $x$ writes a pair of the form $<x, 1>$ to output. There is currently no check to ignore useless input terms, such as single letters or words like "the". Simply filtering by length is not enough: requiring terms be at least length 4 would filter "the" and "an", but also "UTP" and "CD". A future improvement could therefore be implementing a set of checks that prevent terms that are likely to be irrelevant from being processed by the algorithm.
The Reducer here is the same as the reducer for the Authors exercise: it simply aggregates the values into a single total count result per term.
\item \textbf{Job 2: Word Count -} The purpose of this job is to count the total number of terms in each document. The Mapper receives as its input the output for the first job as $<title><word>,<frequency>$, and maps this to $<title>, <word><frequency>$. The Reducer then aggregates the frequency so we get the amount of words for each title. This frequency is important in the TF-IDF calculation so that longer text are not favored over smaller texts. The Reducer finally prints the result as $<title>, <word><n/N>$, where $n$ is the original frequency and $N$ is the total amount of words in the text for that title.
\item \textbf{Job 3: TF-IDF -} This job finalises the entire task by calculating and outputting the required values. The Mapper retrieves the result from Job 2 and maps it to $<word>, <title><n/N>$. Finally, the Reducer first counts the number of titles $d$ that have a certain word in it. If a word is in a smaller amount of documents ($d$) compared to the total amount of documents ($D$) this word becomes more important to describe this document. Next for each $<word><title>$ pair it calculates the tf-idf with the following formula: $$TF-IDF = n/N * log(n/D)$$\todo{TODO: check this formula}
\end{itemize}

\subsection{Results}
First we performed the calculation of the tf-idf on the summaries of the movies xml file. However, because this was a very small file with only 7 titles (of which only 6 has a summary), we took a larger sample from the Dutch Wikipedia. We downloaded the latest dump of Wikipedia~\footnote{\url{http://dumps.wikimedia.org/nlwiki/}}, which is a xml file of around 5.2 GB and contains all text of around 2.5 million articles~\footnote{a small sample of this file with only 1000 pages is included in the input folder}. From this file we extracted the title and the text and calculated the tf-idf for each word in the text. This resulted in \todo{TODO: number pairs, words, etc.}

\todo{TODO finish this and maybe add some stuff about what was difficult or what could be improved.}
\todo{TODO Add stuff about our tests (wikipedia)}


\section{Code listings}
\lstinputlisting[language=PIG, label=lst:pig-ex1, caption=Pig exercise 1, float=h]{pig-exercise1.txt}
\lstinputlisting[language=PIG, label=lst:pig-ex2, caption=Pig exercise 2, float=h]{pig-exercise2.txt}
\lstinputlisting[language=PIG, label=lst:pig-ex3, caption=Pig exercise 3, float=h]{pig-exercise3.txt}
\lstinputlisting[language=PIG, label=lst:pig-ex4, caption=Pig exercise 4, float=h]{pig-exercise4.txt}
\lstinputlisting[language=PIG, label=lst:pig-ex5, caption=Pig exercise 5, float=h]{pig-exercise5.txt}


\end{document}
