\documentclass[11pt]{article}
\input{preamble.tex}

\title{Web Data Management - Assignment 2 \\ Large-Scale Data Management with Hadoop}
\author{Friso Abcouwer - 4019873 \and Matthijs van Dorth - 1265911}

\begin{document}

\maketitle

\section{Introduction}
In this report, we will present our solutions to the exercises in Chapter 19 of the Web Data Management book.
Our Java code can be found in the /src folder in the zip-file we handed in, and our input and output files can be found in the /input and /output folders, respectively.

\section{Combiner Functions}
After implementing the example MapReduce job, making the Combiner was as simple as copying the Reducer and setting it in the Job class. (Looking back, we could also have simply set the Reducer as the Combiner.)
Without the Combiner, the Mapper will send pairs of the form $<author, 1>$ to the Reducer. However, the Combiner intercepts these and instead sends pairs of the form $<author,N>$ to the Reducer, reducing the disk space needed by the Reducer because the intermediate result has already been generated.

\section{Movies}
For the Movies exercise, the first thing we had to do was handle the XML input in such a way that we could easily extract the information we needed from it.
For this, we used the JDOM package~\footnote{JDOM homepage: \url{http://jdom.org/}}, which provides an easy interface for extracting objects and values by name from XML files. 
In our Mapper, this is first done for general movie information, then for information about actors, and finally for information about the director. Movie-actor output lines are all written to their own single key, as are movie-director output lines.  In the Reducer, the values corresponding to the movie-actor key are written to the movie-actor output, and the values corresponding to the movie-director key are written to the movie-director output.

\section{PIGLatin Scripts}
The scripts we used can be found in Listings~\ref{lst:pig-ex1} through~\ref{lst:pig-ex5}.

\section{Inverted File}
For the inverted file project, we based our own solution on the tf-idf for Hadoop tutorial by Marcello de Sales~\footnote{\url{https://code.google.com/p/hadoop-clusternet/wiki/RunningMapReduceExampleTFIDF}}. To obtain our final result, we split the entire task up into 3 Hadoop jobs, which are started from the file \lstinline{IFJob.java}.

\begin{itemize}

\item \textbf{Job 1: Term Frequency - } In this job, the Mapper goes through all of the files in the input directory, and for each term $x$ writes a pair of the form $<x, 1>$ to output. There is currently no check to ignore useless input terms, such as single letters or words like "the". Simply filtering by length is not enough: requiring terms be at least length 4 would filter "the" and "an", but also "UTP" and "CD". A future improvement could therefore be implementing a set of checks that prevent terms that are likely to be irrelevant from being processed by the algorithm.
The Reducer here is the same as the reducer for the Authors exercise: it simply aggregates the values into a single total count result per term.
\item \textbf{Job 2:}
\item \textbf{Job 3:}

\end{itemize}




\lstinputlisting[language=PIG, label=lst:pig-ex1, caption=Pig exercise 1, float=h]{pig-exercise1.txt}
\lstinputlisting[language=PIG, label=lst:pig-ex2, caption=Pig exercise 2, float=h]{pig-exercise2.txt}
\lstinputlisting[language=PIG, label=lst:pig-ex3, caption=Pig exercise 3, float=h]{pig-exercise3.txt}
\lstinputlisting[language=PIG, label=lst:pig-ex4, caption=Pig exercise 4, float=h]{pig-exercise4.txt}
\lstinputlisting[language=PIG, label=lst:pig-ex5, caption=Pig exercise 5, float=h]{pig-exercise5.txt}


\end{document}
